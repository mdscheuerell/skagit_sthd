---
title: Appendix S1. Detailed instructions for replicating analyses and reproducing
  results.
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

***

This is version `r paste0('0.',format(Sys.time(), '%y.%m.%d'))`.

***

```{r set_options, echo=FALSE, cache=FALSE}
options(width = 100)
knitr::opts_chunk$set(message = FALSE)
```

## Requirements
All analyses require the [R software](https://cran.r-project.org/) (v3.2.3) for data retrieval, data processing, and summarizing model results, and the [JAGS software](http://mcmc-jags.sourceforge.net/) (v4.2.0) for Markov chain Monte Carlo (MCMC) simulation. Please note that some of the R code below may not work with older versions of JAGS due to some changes in the ways that arrays are handled.

We also need a few packages that are not included with the base installation of R, so we begin by installing them (if necessary) and then loading them.

```{r load_pkgs, message=FALSE, warning=FALSE}
if(!require("R2jags")) {
  install.packages("R2jags")
  library("R2jags")
}
if(!require("readr")) {
  install.packages("readr")
  library("readr")
}
if(!require("gsl")) {
  install.packages("gsl")
  library("gsl")
}
```

The last thing we'll need a couple of helper functions.

```{r define_Re2prec}
## better round
Re2prec <- function(x,map="round",prec=1) {
  ## 'map' can be round, floor, or ceiling
  ## 'prec' is nearest value (eg, 0.1 means to nearest tenth); default 1 gives normal behavior
  if(prec<=0) { stop("\"prec\" cannot be less than or equal to 0") }
  do.call(map,list(x/prec))*prec
}

## return spring transition index
get_STI <- function(x, day_max=200) {
	return(min(which(x==min(x[1:day_max]))))
}

## colVars; from Gelman
## returns the column-wise variance of a matrix
colVars <- function(a) {
	n <- dim(a)[[1]]
	c <- dim(a)[[2]]
	mm <- matrix(.colMeans(a, n, c), n, c, byrow = TRUE)
	return(.colMeans(((a - mm) ^ 2), n, c) * n / (n - 1))
}

## waic; from Gelman
## computes WAIC based on pointwise log-like
waic <- function(log_lik) {
  S <- nrow(log_lik)
  n <- ncol(log_lik)
  lpd <- log(colMeans(exp(log_lik)))
  p_waic <- colVars(log_lik)
  elpd_waic <- lpd - p_waic
  waic <- -2*elpd_waic
  loo_weights_raw <- 1/exp(log_lik-max(log_lik))
  loo_weights_normalized <- loo_weights_raw /
    matrix(colMeans(loo_weights_raw),nrow=S,ncol=n,byrow=TRUE)
  loo_weights_regularized <- pmin(loo_weights_normalized, sqrt(S))
  elpd_loo <- log(colMeans(exp(log_lik)*loo_weights_regularized) /
                    colMeans(loo_weights_regularized))
  p_loo <- lpd - elpd_loo
  pointwise <- cbind(waic,lpd,p_waic,elpd_waic,p_loo,elpd_loo)
  total <- colSums(pointwise)
  se <- sqrt(n*colVars(pointwise))
  return(list(waic=total["waic"],
              elpd_waic=total["elpd_waic"],
              p_waic=total["p_waic"],
              elpd_loo=total["elpd_loo"],
              p_loo=total["p_loo"],
              pointwise=pointwise,
              total=total,
              se=se))
}
```

## User inputs
We begin by specifying the names of four necessary data files that contain the following information:
 
 1. observed total number of adult spawners (escapement) by year;
 2. observed age composition of adult spawners by year;
 3. observed total harvest by year;
 4. hatchery releases by year.

Let's also define the following parameters, which will be referenced throughout the analysis.

 * `n_yrs`: number of calendar years of data
 * `A`: number of age classes 
 * `M`: number of covariates

```{r get_user_inputs}
## 1. file with escapement data
## [n_yrs x 2] matrix of obs counts; 1st col is calendar yr
fn_esc <- "skagit_sthd_esc.csv"

## 2. file with age comp data
## [n_yrs x (1+A)]; 1st col is calendar yr
fn_age <- "skagit_sthd_age.csv"
## min & max ages
age_min <- 3
age_max <- 8
## years, if any, of age-comp to skip; see below
age_skip <- 2

## 3. file with harvest data
## [n_yrs x 2] matrix of obs catch; 1st col is calendar yr
fn_harv <- "skagit_sthd_catch.csv"

## 4. file with hatchery release data
## [n_yrs x (1+MM)]; 1st col is calendar yr
fn_hrel <- "skagit_sthd_hrel.csv"

## time lags (years) for covariates
flow_lag <- 1
marine_lag <- 2
hrel_lag <- 2

## number of years of forecasts
n_fore <- 1

## file where to save JAGS model
fn_jags <- "skagit_sthd_jags.txt"

## upper threshold for Gelman & Rubin's (1992) potential scale reduction factor (Rhat).
Rhat_thresh <- 1.1

## URL for example data files
## set to NULL if using a local folder/directory
ex_url <- "https://raw.githubusercontent.com/mdscheuerell/skagit_sthd/master/data/"
```

## Loading the fish data
Here we load in the first three data files and do some simple calculations and manipulations. First the spawner data:

```{r get_escapement_data}
## escapement
dat_esc <- read_csv(paste0(ex_url,fn_esc))
## years of data
dat_yrs <- dat_esc$year
## number of years of data
n_yrs <- length(dat_yrs)
## get first & last years
yr_frst <- min(dat_yrs)
yr_last <- max(dat_yrs)
## log of escapement
ln_dat_esc <- c(log(dat_esc$escapement),rep(NA,n_fore))
```

Next the age composition data:

```{r get_age_data}
## age comp data
dat_age <- read_csv(paste0(ex_url,fn_age))
## drop year col & first age_min+age_skip rows
dat_age <- dat_age[-(1:(age_min+age_skip)),-1]
## num of age classes
A <- age_max-age_min+1
## add row(s) of NA's for forecast years
dat_age <- rbind(dat_age,matrix(0,n_fore,A,dimnames=list(n_yrs+seq(n_fore),colnames(dat_age))))
## total num of age obs by cal yr
dat_age[,"sum"] <- apply(dat_age,1,sum)
## row indices for any years with no obs age comp
idx_NA_yrs <- which(dat_age$sum<A,TRUE)
## replace 0's in yrs w/o any obs with NA's
dat_age[idx_NA_yrs,(1:A)] <- NA
## change total in yrs w/o any obs from 0 to A to help dmulti()
dat_age[idx_NA_yrs,"sum"] <- A
## convert class
dat_age <- as.matrix(dat_age)
```

And then the harvest data:

```{r get_harvest}
## harvest
dat_harv <- read_csv(paste0(ex_url,fn_harv))
## drop year col & first age_max rows
dat_harv <- c(dat_harv$catch,rep(0,n_fore))
```

## Loading the covariates

This analysis uses 6 covariates as drivers of the population's instrinic growth rate:

1. Maximum river discharge in winter
2. Minimum river discharge in summer
3. Pacific Decadal Oscillation (PDO)
4. North Pacific Gyre Oscillation (NPGO)
5. Hatchery releases

### River discharge

We begin by getting the daily flow data from the US Geological Service [National Water Information System](http://waterdata.usgs.gov/nwis). We will use the direct link to the gage data from the Skagit River near Mount Vernon, WA (#12178100), beginning with the first year of fish data.

```{r get_flow_url}
## flow site
flow_site <- 12178100
## get URL for flow data from USGS
flow_url <- paste0("https://waterdata.usgs.gov/nwis/dv",
                   "?cb_00060=on",
                   "&format=rdb",
                   "&site_no=",flow_site,
                   "&begin_date=",yr_frst,"-01-01",
                   "&end_date=",yr_last,"-12-31")
```

Next we will retrieve the raw data file and print its metadata.

```{r get_flow_metadata}
## raw flow data from USGS
flow_raw <- read_lines(flow_url)
## lines with metadata
hdr_flow <- which(lapply(flow_raw,grep,pattern="\\#")==1, arr.ind=TRUE)
## print flow metadata
print(flow_raw[hdr_flow],quote=FALSE)
```

Lastly, we will extract the actual flow data for the years of interest and inspect the file contents.

```{r get_flows}
## flow data for years of interest
dat_flow <-  read_tsv(flow_url, col_names = FALSE, col_types = "ciDdc", skip = max(hdr_flow)+2)
colnames(dat_flow) <- unlist(strsplit(tolower(flow_raw[max(hdr_flow)+1]), split="\\s+"))
head(dat_flow)
```

The first 3 columns in the data file are the agency (`agency_cd`), location (`site_no`), and date (`datetime`). The daily flow measurements are in the 4th column (``r grep("[0-9]$",colnames(dat_flow), value=TRUE)``), so we will only keep `datetime` and ``r grep("[0-9]$",colnames(dat_flow), value=TRUE)``, and rename them to `date` and `flow`, respectively. We will also convert the units from "cubic feet per second" to "cubic meters per second".

```{r trim_dat_flow}
## keep only relevant columns
dat_flow <- dat_flow[c("datetime",grep("[0-9]$",colnames(dat_flow),value=TRUE))]
## nicer column names
colnames(dat_flow) <- c("date","flow")
## convert cubic feet to cubic meters
dat_flow$flow <- dat_flow$flow / 35.3147
## flow by year & month
dat_flow$year <- as.integer(format(dat_flow$date,"%Y"))
dat_flow$month <- as.integer(format(dat_flow$date,"%m"))
dat_flow <- dat_flow[,c("year","month","flow")]
```

#### Winter maximum

We are interested in the maximum of the daily peak flows from October through March during the first year that juveniles are rearing in streams. This means we need to combine flow values for the fall of year $t$ with those in the spring of year $t+1$. Therefore, the flow time series will begin in `r yr_frst`; the last year of flow data will be `r yr_last - age_min + n_fore + flow_lag`.

```{r wtr_flow}
## autumn flows in year t
flow_aut <- subset(dat_flow, (month>=10 & month<=12)
                   & year >= yr_frst & year <= yr_last-age_min+n_fore)
## spring flows in year t+1
flow_spr <- subset(dat_flow, (month>=1 & month<=3)
                   & year >= yr_frst+flow_lag & year <= yr_last-age_min+n_fore+flow_lag)
## change spr year index to match aut
flow_spr[,"year"] <- flow_spr[,"year"] - flow_lag
## combined flows indexed to brood year and calculate max flow over time period
dat_flow_wtr <- aggregate(flow ~ year, data=rbind(flow_aut,flow_spr), max)
## change year index to brood year
dat_flow_wtr[,"year"] <- dat_flow_wtr[,"year"] 
## for plotting purpose later
colnames(dat_flow_wtr)[2] <- "Winter"
```

#### Summer minimum

Now we will calculate the minimum flow juveniles would experience during their first summer (June through September).

```{r sum_flow}
## summer flows in year t
flow_sum <- subset(dat_flow, (month>=6 & month<=9)
                   & year >= yr_frst+flow_lag & year <= yr_last-age_min+n_fore+flow_lag)
## change year index to brood year
flow_sum[,"year"] <- flow_sum[,"year"] - flow_lag
## combined flows indexed to brood year and calculate max flow over time period
dat_flow_sum <- aggregate(flow ~ year, data=flow_sum, min)
## for plotting purpose later
colnames(dat_flow_sum)[2] <- "Summer"
```

### North Pacific Gyre Oscillation

We used the monthly NPGO data provided by Emanuele Di Lorenzo, which are available [here](http://www.o3d.org/npgo/npgo.php). We begin by downloading the raw NPGO data and viewing the metadata.

```{r get_NPGO_metadata}
## URL for NPGO data
url_NPGO <- "http://www.o3d.org/npgo/npgo.php"
## raw NPGO data 
NPGO_raw <- read_lines(url_NPGO)
## line with data headers
hdr_NPGO <- which(lapply(NPGO_raw,grep,pattern="YEAR")==1, arr.ind=TRUE)
## print PDO metadata
print(NPGO_raw[seq(hdr_NPGO)],quote=FALSE)
```

Next, we will extract the actual NPGO indices for the years of interest and inspect the file contents. We also want the average NPGO annual index from January 1 through December 31 during the first year that the juvenile steelhead are in the ocean (i.e., during their second year of life). Therefore, we need NPGO values from `yr_frst + marine_lag` = `r yr_frst+marine_lag` through `yr_last - age_min + n_fore + marine_lag` = `r yr_last - age_min + n_fore + marine_lag`.

```{r get_NPGO}
## NPGO data for years of interest
dat_NPGO <- read_table(url_NPGO, col_names = FALSE, skip=hdr_NPGO + (yr_frst-1950)*12, n_max = n_yrs*12)
colnames(dat_NPGO) <- c("year","month","NPGO")
## select only years of interest indexed by brood year 
dat_NPGO <- dat_NPGO[dat_NPGO$year >= yr_frst+marine_lag &
                     dat_NPGO$year <= yr_last-age_min+n_fore+marine_lag,]
dat_NPGO <- aggregate(dat_NPGO$NPGO, by = list(year = dat_NPGO$year), FUN = mean)
dat_NPGO <- data.frame(year=seq(yr_frst,yr_last-age_min+n_fore), NPGO=dat_NPGO[,2])
colnames(dat_NPGO) <- c("year","NPGO")
dat_NPGO
```

### Spring Transition Index

We calculated the spring transition index (STI) from the daily coastal upwelling index (CUI) provided by NOAA's Pacific Fisheries Environmental Laboratory  ([PFEL](https://www.pfeg.noaa.gov/)); you can find more information [here](https://www.pfeg.noaa.gov/products/PFEL/modeled/indices/PFELindices.html). We begin by downloading the raw CUI data and viewing the metadata.

```{r get_CUI_metadata}
## URL for CUI data
url_CUI <- "https://www.pfeg.noaa.gov/products/PFELData/upwell/daily/p06dayac.all"
## raw CUI data from PFEL
CUI_raw <- read_lines(url_CUI)
## line with data headers
hdr_CUI <- which(lapply(CUI_raw,grep,pattern="YYYYMMDD")==1, arr.ind=TRUE)
## print CUI metadata
print(CUI_raw[seq(hdr_CUI-1)],quote=FALSE)
```

```{r get_CUI}
## get daily CUI data
dat_CUI <- read_table(url_CUI, col_names = TRUE, skip=hdr_CUI-1)
## extract year from date
dat_CUI$yr <- gsub("[0-9]{4}$","",dat_CUI$YYYYMMDD)
## select only years of interest
cui <- dat_CUI[dat_CUI$yr >= yr_frst+marine_lag & dat_CUI$yr <= yr_last-age_min+n_fore+marine_lag,]
## calculate cumulative upwelling by year
cum_CUI <- tapply(cui$Index, cui$yr, cumsum)
## calc STI for each year
dat_STI <- data.frame(year=seq(yr_frst,yr_last-age_min+n_fore),STI=sapply(cum_CUI,get_STI))
```

### Hatchery releases

The numbers of hatchery fish released each year is listed in a file on the project site. They have already been lagged 2 years (i.e., brood year + 2) to account for the potential competitive interactions during their juvenile life stage. (We will divide the release number by 1000 for plotting purposes.)

```{r get_hatchery_releases}
dat_hrel <- read_csv(paste0(ex_url,fn_hrel)) 
dat_hrel[,2] <- dat_hrel[,2]/1000
dat_hrel 
```

### Combine all covariates

The last thing we will do is combine the covariates into one file and standardize them to have zero-mean and unit-variance.

```{r combine_covars}
## covariate(s)
dat_cvrs <- Reduce(function(...) merge(..., all=TRUE),
                   list(dat_flow_wtr,dat_flow_sum,dat_NPGO,dat_STI,dat_hrel))
## drop year col
dat_cvrs <- dat_cvrs[,-1] 
## transform the covariates to z-scores
scl_cvrs <- scale(dat_cvrs) 
## total number of covariates
n_cov <- dim(scl_cvrs)[2] 
```

## Specifying model in JAGS

Now we can specify the model in JAGS. Note that the code below is not written to be universally generic with respect to the number of covariates, but rather to emphasize how to incorporate the three we used in this specific application. The important thing is the number of covariate parameters in the `PRIORS` and `LIKELIHOOD` sections (i.e., there must be a unique `c_Name` parameter for each of the _MM_ covariates).

```{r SSSR_in_JAGS}
cat("

model {
	
	##--------
	## PRIORS
	##--------
	## alpha = exp(a) = intrinsic productivity
	alpha ~ dunif(0.1,10);
	mu_Rkr_a <- log(alpha);
	E_Rkr_a <- mu_Rkr_a + sigma_r/(2 - 2*phi^2);

	## strength of dens depend
	beta ~ dunif(0,0.1);

	## covariate effects
	# for(i in 1:n_cov) { gamma[i] ~ dnorm(0,0.001) }
	gamma ~ dnorm(0,0.001)

	## AR(1) coef for proc errors
	phi ~ dunif(-0.999,0.999);
	
	## process variance for recruits model
	sd_r ~ dunif(0.001,20);
	tau_r <- pow(sd_r,-2);
	sigma_r <- pow(sd_r,2);
	
	## innovation in first year
	innov_1 ~ dnorm(0,tau_r*(1-phi*phi));
	
	## obs variance for spawners
	sd_s ~ dunif(0.001,20);
	tau_s <- pow(sd_s,-2);
	sigma_s <- pow(sd_s,2);
	
	## unprojectable early recruits;
	## hyper mean across all popns
	Rec_mu ~ dnorm(0,0.001);
	## hyper SD across all popns
	Rec_sig ~ dunif(0,100);
	## precision across all popns
	Rec_tau <- pow(Rec_sig,-2);
	## multipliers for unobservable total runs
	ttl_run_mu ~ dunif(1,5);
	ttl_run_tau ~ dunif(1,20);
	
	## maturity schedule
	## unif vec for Dirch prior
	for(i in 1:A) { theta[i] <- 1 }
	## hyper-mean for maturity
	pi_eta ~ ddirch(theta);
	## hyper-prec for maturity
	pi_tau ~ dunif(0.001,1e3);
	for(t in 1:(n_yrs-age_min+n_fore)) { pi_vec[t,1:A] ~ ddirch(pi_eta*pi_tau) }
	
	##------------
	## LIKELIHOOD
	##------------
	## 1st brood yr requires different innovation
	## predicted recruits in BY t
	# covar[1] <- inprod(gamma,scl_cvrs[1,]);
	covar[1] <- gamma*mod_cvrs[1];
  ln_Rkr_a[1] <- mu_Rkr_a + covar[1]; 
	E_ln_Rec[1] <- ln_Sp[1] - beta*Sp[1] + ln_Rkr_a[1] + phi*innov_1;
	tot_ln_Rec[1] ~ dnorm(E_ln_Rec[1],tau_r);
	res_ln_Rec[1] <- tot_ln_Rec[1] - E_ln_Rec[1];
	## median of total recruits
	tot_Rec[1] <- exp(tot_ln_Rec[1]);

	## R/S
	ln_RS[1] <- tot_ln_Rec[1] - ln_Sp[1];
		
	## brood-yr recruits by age
	for(a in 1:A) {
		Rec[1,a] <- max(1,tot_Rec[1] * pi_vec[1,a]);
	}
	
	## brood years 2:(n_yrs-age_min)
	for(t in 2:(n_yrs-age_min+n_fore)) {
		## predicted recruits in BY t
		# covar[t] <- inprod(gamma, scl_cvrs[t,]);
	  covar[t] <- gamma*mod_cvrs[t];
    ln_Rkr_a[t] <- mu_Rkr_a + covar[t]; 
		E_ln_Rec[t] <- ln_Sp[t]- beta*Sp[t] + ln_Rkr_a[t] + phi*(res_ln_Rec[t-1] - covar[t-1]);
		tot_ln_Rec[t] ~ dnorm(E_ln_Rec[t],tau_r);
		res_ln_Rec[t] <- tot_ln_Rec[t] - E_ln_Rec[t];
		## median of total recruits
		tot_Rec[t] <- exp(tot_ln_Rec[t]);
		## R/S
		ln_RS[t] <- tot_ln_Rec[t] - ln_Sp[t];
		## brood-yr recruits by age
		for(a in 1:A) {
			Rec[t,a] <- max(1,tot_Rec[t] * pi_vec[t,a]);
		}
	} ## end t loop over year

	## get total cal yr returns for first age_min yrs
	for(i in 1:(age_min+age_skip)) {
		ln_tot_Run[i] ~ dnorm(ttl_run_mu*Rec_mu,Rec_tau/ttl_run_tau);
		tot_Run[i] <- exp(ln_tot_Run[i]);
	}

	## get predicted calendar year returns by age
	## matrix Run has dim [(n_yrs-age_min) x A]
	## step 1: incomplete early broods
	## first cal yr of this grp is first brood yr + age_min + age_skip
	for(i in 1:(age_max-age_min-age_skip)) {
		## projected recruits
		for(a in 1:(i+age_skip)) {
			Run[i,a] <- Rec[(age_skip+i)-a+1,a];
		}
		## imputed recruits
		for(a in (i+1+age_skip):A) {
			lnRec[i,a] ~ dnorm(Rec_mu,Rec_tau);
			Run[i,a] <- exp(lnRec[i,a]);
		}
		## total run size
		tot_Run[i+age_min+age_skip] <- sum(Run[i,1:A]);
		# predicted age-prop vec for multinom
		for(a in 1:A) {
			age_v[i,a] <- Run[i,a] / tot_Run[i+age_min];
		}
		## multinomial for age comp
		dat_age[i,1:A] ~ dmulti(age_v[i,1:A],dat_age[i,A+1]);
		lp_age[i] <- logdensity.multi(dat_age[i,1:A],age_v[i,1:A],dat_age[i,A+1]);
	}
	
	## step 2: info from complete broods
	## first cal yr of this grp is first brood yr + age_max
	for(i in (A-age_skip):(n_yrs-age_min-age_skip+n_fore)) {
		for(a in 1:A) {
			Run[i,a] <- Rec[(age_skip+i)-a+1,a];
		}
		## total run size
		tot_Run[i+age_min+age_skip] <- sum(Run[i,1:A]);
		## predicted age-prop vec for multinom
		for(a in 1:A) {
			age_v[i,a] <- Run[i,a] / tot_Run[i+age_min];
		}
		## multinomial for age comp
		dat_age[i,1:A] ~ dmulti(age_v[i,1:A],dat_age[i,A+1]);
		lp_age[i] <- logdensity.multi(dat_age[i,1:A],age_v[i,1:A],dat_age[i,A+1]);
	}
		
	## get predicted calendar year spawners
	## first cal yr is first brood yr
	for(t in 1:(n_yrs+n_fore)) {
		## obs model for spawners
		Sp[t] <- max(1,tot_Run[t] - dat_harv[t]);
		ln_Sp[t] <- log(Sp[t]);
		ln_dat_esc[t] ~ dnorm(ln_Sp[t], tau_s);
		lp_esc[t] <- logdensity.norm(ln_dat_esc[t],ln_Sp[t], tau_s);
	}
			
} ## end model description

", file=fn_jags)
```

***
## Fitting the model

The last thing we need to do before fitting the model in JAGS is to specify:

1. the data and indices that go into the model;
2. the model parameters and states that we want JAGS to return;
3. the MCMC control parameters.

Please note that the following code takes ~20 min to run on a quad-core machine with 3.5 GHz Intel processors.

```{r start_timer, include=FALSE}
## start timer
timer_start <- proc.time() 
```

```{r JAGS_IO, message=FALSE, warning=FALSE, cache=TRUE}
## data to pass to JAGS
dat_jags <- c("dat_age","ln_dat_esc","dat_harv","mod_cvrs","n_cov",
              "n_yrs","A","age_min","age_max","age_skip","n_fore") 

## 2. model params/states for JAGS to return
#par_jags <- c("alpha","E_Rkr_a","mu_Rkr_a","beta","ln_Rkr_a",
#              "gamma","lp_age","lp_esc",
#              "Sp","Rec","tot_ln_Rec","ln_RS","pi_vec",
#              "sigma_r","sigma_s","res_ln_Rec")
par_jags <- c("gamma","lp_age","lp_esc")

## 3. MCMC control params
## MCMC parameters
mcmc_chains <- 4
mcmc_length <- 5e4
mcmc_burn <- 2.5e4
mcmc_thin <- 50
## total number of MCMC samples
mcmc_samp <- (mcmc_length-mcmc_burn)*mcmc_chains/mcmc_thin

## function to create JAGS inits
init_vals <- function() {
	list(alpha=2, gamma=0.1,
	     beta=1/exp(mean(ln_dat_esc, na.rm=TRUE)),
	     pi_tau=1, pi_eta=rep(1,A),
	     pi_vec=matrix(c(0.01,0.3,0.48,0.15,0.05,0.01),n_yrs-age_min+n_fore,A,byrow=TRUE),
	     Rec_mu=log(1000),
	     Rec_sig=0.1,
	     tot_ln_Rec=rep(log(1000),n_yrs-age_min+n_fore),
	     innov_1=0,
	     phi=0.5)
	}

## fit different Ricker models
rk_fits <- vector("list", n_cov)
for(i in 1:n_cov) {
  ## get covar of interest
  mod_cvrs <- scl_cvrs[, i]
  ## list of model info for JAGS
  mod_jags <- list(data=dat_jags,
                   inits=init_vals,
                   parameters.to.save=par_jags,
                   model.file=fn_jags,
                   n.chains=as.integer(mcmc_chains),
                   n.iter=as.integer(mcmc_length),
                   n.burnin=as.integer(mcmc_burn),
                   n.thin=as.integer(mcmc_thin))
  
  ## fit the model in JAGS & store results
  rk_fits[[i]] <- do.call(jags.parallel, mod_jags)
}
```

```{r stop_timer, include=FALSE}
## stop timer
run_time_in_min <- round(((proc.time()-timer_start)/60)["elapsed"], 1)
cat(run_time_in_min, file="run_time_in_min.txt")
```

## Finding the best model

Let's examine Watanabe's Akaike Information Criterion (WAIC) for each of the models to see which of the covariate scenarios seems to be best supported by the available data.

```{r get_WAIC}
n_mods <- length(rk_fits)
WAIC <- vector("numeric",n_mods)
LOO <- vector("numeric",n_mods)
cc_smry_1 <- matrix(NA,n_mods,3)
colnames(cc_smry_1) <- c("lo","med","up")
## extract log densities from JAGS objects
for(i in 1:n_mods) {
  ldens <- cbind(rk_fits[[i]]$BUGSoutput$sims.list$lp_age, rk_fits[[i]]$BUGSoutput$sims.list$lp_esc)
  WAIC[i] <- waic(ldens)$waic
  LOO[i] <- -2*waic(ldens)$elpd_loo
  cc_smry_1[i,] <- rk_fits[[i]]$BUGSoutput$summary["gamma", c("2.5%","50%","97.5%")]
  # cc_smry_2[i,] <- rk_fits[[i]]$BUGSoutput$summary["c2", c("2.5%","50%","97.5%")]
}
d_WAIC <- round(WAIC-min(WAIC),1)
wt_WAIC <- exp(-0.5*d_WAIC)/sum(exp(-0.5*d_WAIC))
d_LOO <- round(LOO-min(LOO),1)
wt_LOO <- exp(-0.5*LOO)/sum(exp(-0.5*LOO))
#DIC <- round(sapply(rk_fits, function(x) { x$BUGSoutput$DIC }), 1)
#d_DIC <- round(DIC-min(DIC),1)
#wt_DIC <- exp(-0.5*d_DIC)/sum(exp(-0.5*d_DIC))
tbl_WAIC <- data.frame(covar=colnames(scl_cvrs),
                       WAIC=round(WAIC,1),
                       d_WAIC=d_WAIC,
                       wt_WAIC=round(wt_WAIC,3),
                       LOO=round(LOO,1),
                       d_LOO=d_LOO,
                       wt_LOO=round(wt_LOO,3),
#                       DIC=round(DIC,1),
#                       d_DIC=d_DIC,
#                       wt_DIC=round(wt_DIC,3),
                       round(cc_smry_1,2))
# write.csv(tbl_WAIC,  row.names = FALSE,
#           file = "Willamette_Chin_SR_flow_models_mainstem_model_selection_all.csv")
best_idx <- which(tbl_WAIC$d_WAIC==0)
# tbl_WAIC[,!(names(tbl_WAIC) %in% c("lag","WAIC"))]
tbl_WAIC
```

